# Agent System Benchmarks

This document tracks the quality and effectiveness of the KB Labs agent system across different query types and complexity levels.

## üìä Current Status (2026-01-15)

### Baseline (Before Improvements)

| Query Type | Success Rate | Tool Usage | Response Quality | Notes |
|------------|--------------|------------|------------------|-------|
| **Simple Lookup** | 20% | 10% | Poor | Answers from general knowledge, doesn't search codebase |
| **Code Finding** | 30% | 40% | Mediocre | Only searches when explicitly asked "search for X" |
| **Architecture** | 15% | 5% | Poor | Provides generic answers, doesn't explore actual code |
| **Multi-Step** | 10% | 30% | Poor | Gets stuck or loops, doesn't plan approach |
| **Error Recovery** | 5% | 20% | Very Poor | Gives up on first error, no retry logic |
| **AVERAGE** | **16%** | **21%** | **Poor** | Behaves like chatbot, not code assistant |

**Key Issues:**
- ‚ùå Doesn't proactively use tools (Mind RAG, fs:read, etc.)
- ‚ùå Answers from training data instead of codebase
- ‚ùå Requires explicit prompting like "search for X" to trigger tool use
- ‚ùå No structured reasoning or planning
- ‚ùå Gives up immediately on errors
- ‚ùå Loops on complex tasks

### Target (After Phase 1-2 Improvements)

| Query Type | Success Rate | Tool Usage | Response Quality | Notes |
|------------|--------------|------------|------------------|-------|
| **Simple Lookup** | 70% | 80% | Good | Automatically searches codebase first |
| **Code Finding** | 75% | 90% | Good | Uses Mind RAG proactively |
| **Architecture** | 60% | 85% | Good | Explores multiple files, synthesizes |
| **Multi-Step** | 50% | 70% | Acceptable | Plans steps, executes methodically |
| **Error Recovery** | 40% | 60% | Acceptable | Retries with different approach |
| **AVERAGE** | **59%** | **77%** | **Good** | Acts like code assistant, not chatbot |

### Target (After Full Implementation - Phase 1-5)

| Query Type | Success Rate | Tool Usage | Response Quality | Notes |
|------------|--------------|------------|------------------|-------|
| **Simple Lookup** | 90% | 95% | Excellent | Always checks codebase first |
| **Code Finding** | 95% | 98% | Excellent | Smart query classification |
| **Architecture** | 85% | 95% | Excellent | Deep exploration with context |
| **Multi-Step** | 80% | 90% | Very Good | ReAct pattern with memory |
| **Error Recovery** | 70% | 85% | Good | Self-learning recovery |
| **AVERAGE** | **84%** | **93%** | **Excellent** | Intelligent code exploration agent |

---

## üéØ Benchmark Test Suite

### Category 1: Simple Lookup (EASY)

**Goal:** Agent should immediately use Mind RAG or fs:read to find specific code elements.

#### Test 1.1: Class Definition Lookup
**Query:** "What is the VectorStore interface?"

**Expected Behavior:**
1. Immediately calls `mind:rag-query` with "VectorStore interface definition"
2. Reads relevant files
3. Provides accurate answer with source references

**Current Behavior (Baseline):**
- ‚ùå Answers from general knowledge: "VectorStore is typically an interface for..."
- ‚ùå Doesn't search codebase
- **Score: 0/10**

**Target Behavior (Phase 1-2):**
- ‚úÖ Calls Mind RAG automatically
- ‚úÖ Provides actual definition from codebase
- **Score: 7/10**

**Target Behavior (Full):**
- ‚úÖ Classifies as "simple lookup"
- ‚úÖ Uses optimized query weights
- ‚úÖ Provides definition + usage examples
- **Score: 9/10**

---

#### Test 1.2: Find File Location
**Query:** "Where is the agent executor implemented?"

**Expected Behavior:**
1. Calls `mind:rag-query` with "agent executor implementation"
2. Reports exact file path
3. Shows key methods/structure

**Current Behavior (Baseline):**
- ‚ùå Generic answer: "Agent executors are typically in src/executor..."
- ‚ùå No actual file search
- **Score: 0/10**

**Target Behavior (Phase 1-2):**
- ‚úÖ Searches and finds file
- ‚úÖ Reports actual path
- **Score: 7/10**

**Target Behavior (Full):**
- ‚úÖ Finds file + shows structure
- ‚úÖ Lists available methods
- **Score: 9/10**

---

### Category 2: Code Finding (MEDIUM)

**Goal:** Agent should explore codebase to understand implementations.

#### Test 2.1: Feature Implementation
**Query:** "How does loop detection work in agents?"

**Expected Behavior:**
1. Searches for "loop detection" in agent code
2. Reads loop-detector.ts
3. Explains the 3 strategies with code references

**Current Behavior (Baseline):**
- ‚ùå Generic explanation of loop detection concepts
- ‚ùå Doesn't read actual implementation
- **Score: 1/10** (at least topic is correct)

**Target Behavior (Phase 1-2):**
- ‚úÖ Finds and reads loop-detector.ts
- ‚úÖ Explains actual implementation
- **Score: 7/10**

**Target Behavior (Full):**
- ‚úÖ Complete explanation with code snippets
- ‚úÖ Links to related files
- **Score: 9/10**

---

#### Test 2.2: Error Handling Flow
**Query:** "What happens when a tool execution fails?"

**Expected Behavior:**
1. Searches for tool execution error handling
2. Reads tool-executor.ts
3. Traces error flow through agent-executor.ts
4. Explains current (poor) error handling

**Current Behavior (Baseline):**
- ‚ùå Generic: "Usually errors are caught and logged..."
- ‚ùå No codebase exploration
- **Score: 0/10**

**Target Behavior (Phase 1-2):**
- ‚úÖ Finds actual error handling code
- ‚úÖ Explains what currently happens
- **Score: 6/10**

**Target Behavior (Full):**
- ‚úÖ Complete trace through execution flow
- ‚úÖ Identifies improvement opportunities
- **Score: 9/10**

---

### Category 3: Architecture Understanding (HARD)

**Goal:** Agent should explore multiple files to understand system design.

#### Test 3.1: End-to-End Flow
**Query:** "Explain how an agent processes a task from start to finish"

**Expected Behavior:**
1. Classifies as "architecture" query
2. Searches for agent execution flow
3. Reads: agent-executor.ts, tool-executor.ts, loop-detector.ts
4. Synthesizes complete flow diagram
5. Identifies current limitations

**Current Behavior (Baseline):**
- ‚ùå Generic agent architecture from training data
- ‚ùå Doesn't explore actual codebase
- **Score: 0/10**

**Target Behavior (Phase 1-2):**
- ‚úÖ Reads key files
- ‚úÖ Explains current flow
- ‚ö†Ô∏è May miss some connections
- **Score: 5/10**

**Target Behavior (Full):**
- ‚úÖ Complete architecture understanding
- ‚úÖ Multi-file synthesis
- ‚úÖ Identifies patterns and anti-patterns
- **Score: 8/10**

---

#### Test 3.2: Design Decisions
**Query:** "Why does the agent system use native tool calling instead of text-based?"

**Expected Behavior:**
1. Searches for tool calling implementation
2. Reads agent-executor.ts and related docs
3. Finds architecture decisions (ADRs if available)
4. Explains rationale with evidence

**Current Behavior (Baseline):**
- ‚ùå Generic comparison of approaches
- ‚ùå No reference to actual codebase decisions
- **Score: 0/10**

**Target Behavior (Phase 1-2):**
- ‚úÖ Finds implementation details
- ‚úÖ Explains actual approach used
- **Score: 6/10**

**Target Behavior (Full):**
- ‚úÖ Complete rationale with code evidence
- ‚úÖ Compares alternatives
- **Score: 8/10**

---

### Category 4: Multi-Step Tasks (VERY HARD)

**Goal:** Agent should break down complex tasks and execute methodically.

#### Test 4.1: Add New Feature
**Query:** "Add a timeout mechanism to tool execution"

**Expected Behavior:**
1. Researches current tool execution (tool-executor.ts)
2. Plans implementation steps
3. Identifies files to modify
4. Proposes implementation with code
5. Considers error cases

**Current Behavior (Baseline):**
- ‚ùå Provides generic timeout implementation
- ‚ùå Doesn't research actual codebase
- ‚ùå No plan or step breakdown
- **Score: 0/10**

**Target Behavior (Phase 1-2):**
- ‚úÖ Reads actual implementation
- ‚úÖ Creates basic plan
- ‚ö†Ô∏è May miss edge cases
- **Score: 4/10**

**Target Behavior (Full):**
- ‚úÖ Complete analysis with ReAct pattern
- ‚úÖ Step-by-step implementation plan
- ‚úÖ Considers all edge cases
- ‚úÖ Validates approach
- **Score: 8/10**

---

#### Test 4.2: Debug Complex Issue
**Query:** "Agent gets stuck in loops when Mind RAG returns no results. Fix this."

**Expected Behavior:**
1. Researches loop detection (loop-detector.ts)
2. Researches Mind RAG integration
3. Identifies root cause
4. Plans fix across multiple files
5. Implements with retry logic

**Current Behavior (Baseline):**
- ‚ùå Suggests generic retry logic
- ‚ùå Doesn't analyze actual issue
- ‚ùå No codebase exploration
- **Score: 0/10**

**Target Behavior (Phase 1-2):**
- ‚úÖ Finds relevant code
- ‚úÖ Identifies issue
- ‚ö†Ô∏è Basic fix proposal
- **Score: 3/10**

**Target Behavior (Full):**
- ‚úÖ Complete root cause analysis
- ‚úÖ Multi-file fix plan
- ‚úÖ Error recovery strategy
- ‚úÖ Tests approach
- **Score: 7/10**

---

### Category 5: Error Recovery (EXTREME)

**Goal:** Agent should recover gracefully from tool failures.

#### Test 5.1: Tool Failure Recovery
**Query:** "Explain the state broker architecture" (when Mind RAG is down)

**Expected Behavior:**
1. Tries Mind RAG ‚Üí fails
2. Falls back to grep/find
3. Reads found files
4. Still provides answer

**Current Behavior (Baseline):**
- ‚ùå Fails immediately
- ‚ùå Says "I couldn't search the codebase"
- ‚ùå No fallback attempt
- **Score: 0/10**

**Target Behavior (Phase 1-2):**
- ‚úÖ Detects failure
- ‚ö†Ô∏è May give up or need prompting
- **Score: 2/10**

**Target Behavior (Full):**
- ‚úÖ Automatic fallback strategy
- ‚úÖ Tries alternative tools
- ‚úÖ Learns from failure
- **Score: 7/10**

---

#### Test 5.2: Invalid Input Recovery
**Query:** "Read /nonexistent/file.ts and explain it"

**Expected Behavior:**
1. Tries to read file ‚Üí fails
2. Searches for similar file names
3. Asks clarification or suggests alternatives
4. Doesn't give up

**Current Behavior (Baseline):**
- ‚ùå Error message and stops
- ‚ùå No recovery attempt
- **Score: 0/10**

**Target Behavior (Phase 1-2):**
- ‚úÖ Detects failure
- ‚úÖ Tries to find similar files
- **Score: 4/10**

**Target Behavior (Full):**
- ‚úÖ Smart recovery with search
- ‚úÖ Suggests alternatives
- ‚úÖ Learns common mistakes
- **Score: 8/10**

---

## üìà Metrics Definitions

### Success Rate
- **0-20%**: Agent fails or provides generic/incorrect answer
- **21-40%**: Agent attempts but result is incomplete or wrong
- **41-60%**: Agent provides acceptable answer with issues
- **61-80%**: Agent provides good answer with minor gaps
- **81-100%**: Agent provides excellent, accurate, complete answer

### Tool Usage Rate
Percentage of queries where agent proactively uses appropriate tools (Mind RAG, fs:read, etc.) without explicit prompting.

### Response Quality
- **Poor (0-3)**: Generic knowledge, no codebase references
- **Mediocre (4-5)**: Some codebase search, incomplete
- **Acceptable (6)**: Good codebase exploration, minor gaps
- **Good (7-8)**: Thorough exploration, accurate synthesis
- **Excellent (9-10)**: Deep analysis, perfect execution

---

## üîß Running Benchmarks

### Manual Testing
```bash
# Test simple lookup
pnpm kb agent:run --agentId=mind-assistant --task="What is the VectorStore interface?"

# Test code finding
pnpm kb agent:run --agentId=mind-assistant --task="How does loop detection work in agents?"

# Test architecture
pnpm kb agent:run --agentId=mind-assistant --task="Explain how an agent processes a task from start to finish"
```

### Automated Benchmark Script (TODO)
```bash
# Run all benchmarks
./kb-labs-agents/scripts/run-benchmarks.sh

# Run specific category
./kb-labs-agents/scripts/run-benchmarks.sh --category="simple-lookup"

# Compare before/after
./kb-labs-agents/scripts/run-benchmarks.sh --compare
```

---

## üéØ Success Criteria

### Phase 1-2 (ReAct + Memory) Goals:
- ‚úÖ **Average Success Rate: >50%** (from 16%)
- ‚úÖ **Tool Usage Rate: >70%** (from 21%)
- ‚úÖ **Simple Lookup: >70%** (from 20%)
- ‚úÖ **Code Finding: >70%** (from 30%)
- ‚úÖ **Architecture: >50%** (from 15%)

### Full Implementation (Phase 1-5) Goals:
- ‚úÖ **Average Success Rate: >80%** (5x improvement)
- ‚úÖ **Tool Usage Rate: >90%** (4.3x improvement)
- ‚úÖ **All Categories: >70%**
- ‚úÖ **Simple/Code Finding: >90%**
- ‚úÖ **Error Recovery: >60%** (12x improvement)

---

## üìù Test Execution Log

### Test Run Template

```markdown
**Date:** 2026-01-XX
**Agent Version:** [Baseline/Phase-1/Phase-2/Full]
**Model:** GPT-4o-mini / GPT-4o / Claude Opus

| Test ID | Query | Success? | Tools Used | Quality | Notes |
|---------|-------|----------|------------|---------|-------|
| 1.1 | VectorStore interface | ‚ùå | 0/1 | 0/10 | Generic answer |
| 1.2 | Agent executor location | ‚ùå | 0/1 | 0/10 | Didn't search |
| ... | ... | ... | ... | ... | ... |

**Summary:**
- Success Rate: X%
- Tool Usage: X%
- Average Quality: X/10
```

---

## üîç Analysis Notes

### Current Issues (Baseline)

**Root Cause #1: Passive Tool Usage**
- Agent only uses tools when explicitly told "search for X"
- Prefers answering from training data
- **Fix:** ReAct pattern forces tool-first approach

**Root Cause #2: No Task Planning**
- Jumps to answer without exploration
- Doesn't break down complex queries
- **Fix:** Task Classification + Multi-Step Planning

**Root Cause #3: Error Intolerance**
- Gives up on first tool failure
- No fallback strategies
- **Fix:** Error Recovery + Self-Learning

**Root Cause #4: No Memory**
- Repeats same failed attempts
- Forgets previous findings
- **Fix:** Execution Memory

---

## üìö Related Documentation

- [Agent Improvements Roadmap](./AGENT-IMPROVEMENTS-ROADMAP.md)
- [Mind RAG Benchmarks](../kb-labs-mind/packages/mind-engine/BENCHMARKS.md)
- [Mind RAG ADR-0033: Adaptive Search Weights](../kb-labs-mind/docs/adr/0033-adaptive-search-weights.md)
- [Agent Executor Source](../kb-labs-agents/packages/agent-core/src/executor/agent-executor.ts)

---

**Last Updated:** 2026-01-15
**Status:** Baseline Established
**Next Steps:**
1. Implement Phase 1 (ReAct Pattern)
2. Run initial benchmark tests
3. Track improvements iteratively
